---
title: "Machine Learning Project"
author: "Jason"
date: "9/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(caret)
library(lattice)
library(ggplot2)
library(rpart)

# Download the dataset
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile = "~/pml-training.csv")

# Read in the data file.
dataset <- read.csv(file = "~/pml-training.csv")

colnames <- names(dataset)
```

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

## Approach
The goal of this project is to predict the manner in which the participates did the exercise. This is the "classe" variable in the training set. We will consider use any of the other variables to predict with. 

## Training Dataset
In order to build and validate the model, we seperated the training set into a training set and a testing set. 70% of the data was used for the training set and 30% of the data wa set aside for the training set. We can use the test dataset to measure the expected out of sample error rate.

The following columnns exist in the dataset: `r paste(colnames, collapse =", ")`.

## Feature Selection
Upon inspection of the dataset, the were a number of columns that are not candidates for features in the model.

```{r}
# Predict column
predict.column <- "classe"

# Columns that are considered id columns,
exclude.id.columns <- c("X",
                        "user_name",
                        "raw_timestamp_part_1",
                        "raw_timestamp_part_2",
                        "cvtd_timestamp",
                        "new_window",
                        "num_window")

# Missing value columns
exclude.na.columns <- c("kurtosis_roll_belt",
                        "kurtosis_picth_belt",
                        "kurtosis_yaw_belt",
                        "skewness_roll_belt",
                        "skewness_roll_belt.1",
                        "skewness_yaw_belt",
                        "max_roll_belt",
                        "max_picth_belt",
                        "max_yaw_belt",
                        "min_roll_belt",
                        "min_pitch_belt",
                        "min_yaw_belt",
                        "amplitude_roll_belt",
                        "amplitude_pitch_belt",
                        "amplitude_yaw_belt",
                        "var_total_accel_belt",
                        "avg_roll_belt",
                        "stddev_roll_belt",
                        "var_roll_belt",
                        "avg_pitch_belt",
                        "stddev_pitch_belt",
                        "var_pitch_belt",
                        "avg_yaw_belt",
                        "stddev_yaw_belt",
                        "var_yaw_belt",
                        "var_accel_arm",
                        "avg_roll_arm",
                        "stddev_roll_arm",
                        "var_roll_arm",
                        "avg_pitch_arm",
                        "stddev_pitch_arm",
                        "var_pitch_arm",
                        "avg_yaw_arm",
                        "stddev_yaw_arm",
                        "var_yaw_arm",
                        "kurtosis_roll_arm",
                        "kurtosis_picth_arm",
                        "kurtosis_yaw_arm",
                        "skewness_roll_arm",
                        "skewness_pitch_arm",
                        "skewness_yaw_arm",
                        "max_roll_arm",
                        "max_picth_arm",
                        "max_yaw_arm",
                        "min_roll_arm",
                        "min_pitch_arm",
                        "min_yaw_arm",
                        "amplitude_roll_arm",
                        "amplitude_pitch_arm",
                        "amplitude_yaw_arm",
                        "kurtosis_roll_dumbbell",
                        "kurtosis_picth_dumbbell",
                        "kurtosis_yaw_dumbbell",
                        "skewness_roll_dumbbell",
                        "skewness_pitch_dumbbell",
                        "skewness_yaw_dumbbell",
                        "max_roll_dumbbell",
                        "max_picth_dumbbell",
                        "max_yaw_dumbbell",
                        "min_roll_dumbbell",
                        "min_pitch_dumbbell",
                        "min_yaw_dumbbell",
                        "amplitude_roll_dumbbell",
                        "amplitude_pitch_dumbbell",
                        "amplitude_yaw_dumbbell",
                        "var_accel_dumbbell",
                        "avg_roll_dumbbell",
                        "stddev_roll_dumbbell",
                        "var_roll_dumbbell",
                        "avg_pitch_dumbbell",
                        "stddev_pitch_dumbbell",
                        "var_pitch_dumbbell",
                        "avg_yaw_dumbbell",
                        "stddev_yaw_dumbbell",
                        "var_yaw_dumbbell",
                        "kurtosis_roll_forearm",
                        "kurtosis_picth_forearm",
                        "kurtosis_yaw_forearm",
                        "skewness_roll_forearm",
                        "skewness_pitch_forearm",
                        "skewness_yaw_forearm",
                        "max_roll_forearm",
                        "max_picth_forearm",
                        "max_yaw_forearm",
                        "min_roll_forearm",
                        "min_pitch_forearm",
                        "min_yaw_forearm",
                        "amplitude_roll_forearm",
                        "amplitude_pitch_forearm",
                        "amplitude_yaw_forearm",
                        "var_accel_forearm",
                        "avg_roll_forearm",
                        "stddev_roll_forearm",
                        "var_roll_forearm",
                        "avg_pitch_forearm",
                        "stddev_pitch_forearm",
                        "var_pitch_forearm",
                        "avg_yaw_forearm",
                        "stddev_yaw_forearm",
                        "var_yaw_forearm")

# Now that we have the lists of columns to exclude, let's remove those columns here.
feature.columns <- setdiff(colnames, exclude.na.columns)
feature.columns <- setdiff(feature.columns, exclude.id.columns)
feature.columns <- setdiff(feature.columns, predict.column)
```

### Prediction Column
The prediction column `r predict.column` will need to be removed as a candidate feature for the model.

### Identifier Columns
The following columns are considered identifier columns and are not candidate features for the model: `r paste(exclude.id.columns, collapse=", ")`.

### Missing value Columns
The following columns have missing values for all rows and therefore should be removed as candidate features for the model: `r paste(exclude.na.columns, collapse=", ")`.

### Candidate Features
The following remaining `r length(feature.columns)` columns are the candidate features we will consider for our model: `r paste(feature.columns, collapse=", ")`.

## Model Creation
In order to build the most accurate model, we will choose to use the Random Forest algorithm. We will leverage the caret package to provide convenience for cross validation and predicting. We will use k-fold cross validation with 5 folds in order to produce a more accurate model.

```{r}
# Now, we need to split the dataset into a testing set and a training set.
set.seed(3433)

inTrain = createDataPartition(dataset$classe, p = .7)[[1]]
training = dataset[ inTrain,]
testing = dataset[-inTrain,]


# Now, we will train a model with random forest.
fmla <- as.formula(paste(paste(predict.column, "~ "), paste(feature.columns, collapse= "+")))

# Train the model using random forest and 5 k-fold cross validation.
model.rf <- train(fmla, 
                  data=training, 
                  method="rf",
                  trControl=trainControl(method="cv", number=5))
```

## Evaluation
We can now test the accuracy of the model by calcuating the out of sample accuracy rate and error rate. We can do so because we only trained the model with 70% of the dataset and left 30% for testing.

```{r}
# Now, let's use the model to predict the test outcomes.
test.rf <- predict(model.rf, newdata = testing)

# Determine the out of sample error rate of the model based on the test cases set aside.
accuracy <- sum(test.rf == testing$classe) / length(testing$classe)
errorrate <- 1 - accuracy
```

The accuracy of the random forest model is `r accuracy` and the error rate is `r errorrate`.

## Conclusion
By applying appropriate feature selection techniques and applying the the random forest modeling algorithm with k-fold cross validation, we were able to produce a highly accurate predictive model.